\documentclass [11pt]{article}
%Needed for fancy maths stuff
\usepackage{amsmath}
%Needed for Wits Style
\usepackage{url}
\usepackage{natbib} \input{natbib-add}
\bibpunct{[}{]}{;}{a}{}{}

\title{Building Bayesian Influence Ontologies\\Literature Review}
\author{Tamlin Love\\1438243}
\date{\today}

\usepackage[margin=1in]{geometry} %Margin size

\begin{document}
\maketitle
\section{Introduction}
In Section \ref{SimilarityMetrics}, we discuss a variety of classical approaches used to measure similarity. In Section \ref{BayesianNetworks}, we discuss the concept of a Bayesian Network. In Section \ref{ScoreBasedStructureLearning}, we discuss methods used to learn Bayesian network structures and evaluate them. In Section \ref{BayesianSimilarity}, we discuss the application of Bayesian Networks to measuring similarity.
\section{Similarity Metrics}\label{SimilarityMetrics}
\section{Bayesian Networks}\label{BayesianNetworks}
When considering a joint probability distribution across $n$ random variables, classical probability states that the number of parameters needed to represent the distribution grows exponentially in $n$ \citep{koller09}. Even in the simple case of binary variables, we would still need $2^n -1$ parameters to describe the distribution. This is clearly unfeasible for practical applications, in which the number of random variables can grow very large.
\section{Score-Based Structure Learning}\label{ScoreBasedStructureLearning}
\section{Bayesian Similarity}\label{BayesianSimilarity}
\nocite*{â€¢}
\bibliographystyle{named-wits}
\bibliography{annot}
\end{document}